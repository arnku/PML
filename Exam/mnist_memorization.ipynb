{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial \n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed \n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(input_dim, output_dim)\n",
    "  def forward(self, x):\n",
    "    return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "  def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "    \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "    Args:\n",
    "      marginal_prob_std: A function that takes time t and gives the standard\n",
    "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "      channels: The number of channels for feature maps of each resolution.\n",
    "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # Gaussian random feature embedding layer for time\n",
    "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "         nn.Linear(embed_dim, embed_dim))\n",
    "    # Encoding layers where the resolution decreases\n",
    "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "    self.dense1 = Dense(embed_dim, channels[0])\n",
    "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "    self.dense2 = Dense(embed_dim, channels[1])\n",
    "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense3 = Dense(embed_dim, channels[2])\n",
    "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "    self.dense4 = Dense(embed_dim, channels[3])\n",
    "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
    "\n",
    "    # Decoding layers where the resolution increases\n",
    "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense5 = Dense(embed_dim, channels[2])\n",
    "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense6 = Dense(embed_dim, channels[1])\n",
    "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense7 = Dense(embed_dim, channels[0])\n",
    "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "    \n",
    "    # The swish activation function\n",
    "    self.act = lambda x: x * torch.sigmoid(x)\n",
    "    self.marginal_prob_std = marginal_prob_std\n",
    "  \n",
    "  def forward(self, x, t): \n",
    "    # Obtain the Gaussian random feature embedding for t   \n",
    "    embed = self.act(self.embed(t))    \n",
    "    # Encoding path\n",
    "    h1 = self.conv1(x)    \n",
    "    ## Incorporate information from t\n",
    "    h1 += self.dense1(embed)\n",
    "    ## Group normalization\n",
    "    h1 = self.gnorm1(h1)\n",
    "    h1 = self.act(h1)\n",
    "    h2 = self.conv2(h1)\n",
    "    h2 += self.dense2(embed)\n",
    "    h2 = self.gnorm2(h2)\n",
    "    h2 = self.act(h2)\n",
    "    h3 = self.conv3(h2)\n",
    "    h3 += self.dense3(embed)\n",
    "    h3 = self.gnorm3(h3)\n",
    "    h3 = self.act(h3)\n",
    "    h4 = self.conv4(h3)\n",
    "    h4 += self.dense4(embed)\n",
    "    h4 = self.gnorm4(h4)\n",
    "    h4 = self.act(h4)\n",
    "\n",
    "    # Decoding path\n",
    "    h = self.tconv4(h4)\n",
    "    ## Skip connection from the encoding path\n",
    "    h += self.dense5(embed)\n",
    "    h = self.tgnorm4(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "    h += self.dense6(embed)\n",
    "    h = self.tgnorm3(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "    h += self.dense7(embed)\n",
    "    h = self.tgnorm2(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "    # Normalize output\n",
    "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016, \n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "    \n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arn/Documents/PML/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, network, T=100, beta_1=1e-4, beta_T=2e-2):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1 \n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (self._network(x.reshape(-1, 1, 28, 28), \n",
    "                                                   (t.reshape(-1)/T))\n",
    "                                    ).reshape(-1, 28*28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T+1))\n",
    "        self.register_buffer(\"alpha\", 1-self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "        \n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        '''\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon. \n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index \n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        ''' \n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t])*x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        \n",
    "        return mean + std*epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        mean =  1./torch.sqrt(self.alpha[t]) * (xt - (self.beta[t])/torch.sqrt(1-self.alpha_bar[t])*self.network(xt, t)) \n",
    "        std = torch.where(t>0, torch.sqrt(((1-self.alpha_bar[t-1]) / (1-self.alpha_bar[t]))*self.beta[t]), 0)\n",
    "        \n",
    "        return mean + std*epsilon\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, deterministic=True):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 and not deterministic else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)            \n",
    "            xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    \n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value            \n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t\n",
    "        t = torch.randint(1, self.T, (x0.shape[0],1)).to(x0.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "\n",
    "        return -nn.MSELoss(reduction='mean')(epsilon, self.network(xt, t))\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "        return -self.elbo_simple(x0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, dataloader, epochs, device, ema=True, per_epoch_callback=None):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(model, device=device, decay=1.0 - ema_alpha)                \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x,_) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"⠀{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\", lr=f\"{scheduler.get_last_lr()[0]:.2E}\")\n",
    "            progress_bar.update()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter%ema_steps==0:\n",
    "                    ema_model.update_parameters(model)                \n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            if epoch % 3000 == 2999:\n",
    "                per_epoch_callback(ema_model.module if ema else model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "Training:   3%|▎         | 3000/100000 [00:23<11:51, 136.41it/s, epoch=3000/100000, loss=⠀      0.4734, lr=7.41E-04]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 4885/100000 [00:39<12:15, 129.24it/s, epoch=4886/100000, loss=⠀      0.0214, lr=6.13E-04] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    103\u001b[39m             exit(\u001b[32m0\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Call training loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_epoch_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreporter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(mse_list)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Plot MSE over training\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, scheduler, dataloader, epochs, device, ema, per_epoch_callback)\u001b[39m\n\u001b[32m     44\u001b[39m x = x.to(device)\n\u001b[32m     45\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m loss.backward()\n\u001b[32m     48\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mDDPM.loss\u001b[39m\u001b[34m(self, x0)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x0):\n\u001b[32m    150\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m    Loss function. Just the negative of the ELBO.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43melbo_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m.mean()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mDDPM.elbo_simple\u001b[39m\u001b[34m(self, x0)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# TODO: Forward diffusion to produce image at step t\u001b[39;00m\n\u001b[32m    144\u001b[39m xt = \u001b[38;5;28mself\u001b[39m.forward_diffusion(x0, t, epsilon)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -nn.MSELoss(reduction=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)(epsilon, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mDDPM.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(x, t)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Normalize time input before evaluating neural network\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Reshape input into image format and normalize time value before sending it to network model\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m._network = network\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mself\u001b[39m.network = \u001b[38;5;28;01mlambda\u001b[39;00m x, t: (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                                           \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m/\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m                             ).reshape(-\u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m*\u001b[32m28\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Total number of time steps\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.T = T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mScoreNet.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m    104\u001b[39m h = \u001b[38;5;28mself\u001b[39m.act(h)\n\u001b[32m    105\u001b[39m h = \u001b[38;5;28mself\u001b[39m.tconv3(torch.cat([h, h3], dim=\u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m h += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense6\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m h = \u001b[38;5;28mself\u001b[39m.tgnorm3(h)\n\u001b[32m    108\u001b[39m h = \u001b[38;5;28mself\u001b[39m.act(h)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mDense.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[..., \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1778\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1789\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1784\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1787\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1788\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1792\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Only train on a single MNIST image for memorization\n",
    "\n",
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100000\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020, \n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),    # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x-0.5)*2.0),                    # Map from [0,1] -> [-1, -1]\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])\n",
    "\n",
    "\n",
    "# Only have a single image in the dataset for memorization\n",
    "dataset_raw = datasets.MNIST('./mnist_data', download=True, train=True, transform=transform)\n",
    "fixed_image, fixed_label = dataset_raw[0]\n",
    "dataloader_train = torch.utils.data.DataLoader([(fixed_image, fixed_label)],\n",
    "                                               batch_size=1,\n",
    "                                                shuffle=True)\n",
    "\n",
    "# display the image we are going to memorize\n",
    "ground_image, _ = fixed_image, fixed_label\n",
    "plt.gca().set_axis_off()\n",
    "plt.imshow(transforms.functional.to_pil_image((ground_image.reshape(28,28)+1)/2), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ground_image = ((ground_image.flatten()+1)/2).to(device)\n",
    "\n",
    "# Construct Unet\n",
    "# The original ScoreNet expects a function with std for all the\n",
    "# different noise levels, such that the output can be rescaled.\n",
    "# Since we are predicting the noise (rather than the score), we\n",
    "# ignore this rescaling and just set std=1 for all t.\n",
    "# mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "#mnist_mlp = SimpleMLP((lambda t: torch.ones(1).to(device)))\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "    \n",
    "# Construct model\n",
    "model = DDPM(mnist_unet, T=T).to(device)\n",
    "\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "mse_list = []\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"\n",
    "    Callback function used for plotting images during training.\n",
    "    Also checks how well the model has memorized the image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 100\n",
    "        samples = model.sample((nsamples,28*28))\n",
    "        \n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples+1)/2 \n",
    "        mse = nn.MSELoss(reduction='none')(samples, ground_image.expand_as(samples)).mean(dim=1)\n",
    "\n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "        # Compute MSE with ground truth image\n",
    "        # Find MSE for all samples\n",
    "        \n",
    "        best_mse = torch.min(mse).item()\n",
    "        mse_list.append(best_mse)\n",
    "\n",
    "        # Plot top 5 samples with MSE values overlaid on the images\n",
    "        topk = torch.topk(mse, 5, largest=False)\n",
    "        best_indices = topk.indices\n",
    "        best_samples = samples[best_indices].reshape(-1, 1, 28, 28)\n",
    "\n",
    "        best_mse = torch.min(mse).item()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "        for i, ax in enumerate(axes):\n",
    "            img = transforms.functional.to_pil_image(best_samples[i])\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.axis('off')\n",
    "            ax.text(2, 26, f\"{topk.values[i].item():.12f}\", color='green',\n",
    "                fontsize=12, ha='left', va='bottom',\n",
    "                bbox=dict(facecolor='black', alpha=0.7, pad=1))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if best_mse < 1e-5:\n",
    "            print(f\"Achieved MSE {best_mse:.12f}, stopping training.\")\n",
    "            exit(0)\n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train, \n",
    "      epochs=epochs, device=device, ema=False, per_epoch_callback=reporter)\n",
    "\n",
    "print(mse_list)\n",
    "# Plot MSE over training\n",
    "plt.plot(mse_list)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs (x3000)\")\n",
    "plt.ylabel(\"Best MSE\")\n",
    "plt.title(\"MSE of best sample during training\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "​",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:  24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": " 1393/5900 [05:15&lt;16:04,  4.67it/s, epoch=12/50, loss=⠀   2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
