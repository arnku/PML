{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arn/Documents/PML/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed \n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(input_dim, output_dim)\n",
    "  def forward(self, x):\n",
    "    return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "  def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "    \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "    Args:\n",
    "      marginal_prob_std: A function that takes time t and gives the standard\n",
    "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "      channels: The number of channels for feature maps of each resolution.\n",
    "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # Gaussian random feature embedding layer for time\n",
    "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "         nn.Linear(embed_dim, embed_dim))\n",
    "    # Encoding layers where the resolution decreases\n",
    "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "    self.dense1 = Dense(embed_dim, channels[0])\n",
    "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "    self.dense2 = Dense(embed_dim, channels[1])\n",
    "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense3 = Dense(embed_dim, channels[2])\n",
    "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "    self.dense4 = Dense(embed_dim, channels[3])\n",
    "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
    "\n",
    "    # Decoding layers where the resolution increases\n",
    "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense5 = Dense(embed_dim, channels[2])\n",
    "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense6 = Dense(embed_dim, channels[1])\n",
    "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense7 = Dense(embed_dim, channels[0])\n",
    "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "    \n",
    "    # The swish activation function\n",
    "    self.act = lambda x: x * torch.sigmoid(x)\n",
    "    self.marginal_prob_std = marginal_prob_std\n",
    "  \n",
    "  def forward(self, x, t): \n",
    "    # Obtain the Gaussian random feature embedding for t   \n",
    "    embed = self.act(self.embed(t))    \n",
    "    # Encoding path\n",
    "    h1 = self.conv1(x)    \n",
    "    ## Incorporate information from t\n",
    "    h1 += self.dense1(embed)\n",
    "    ## Group normalization\n",
    "    h1 = self.gnorm1(h1)\n",
    "    h1 = self.act(h1)\n",
    "    h2 = self.conv2(h1)\n",
    "    h2 += self.dense2(embed)\n",
    "    h2 = self.gnorm2(h2)\n",
    "    h2 = self.act(h2)\n",
    "    h3 = self.conv3(h2)\n",
    "    h3 += self.dense3(embed)\n",
    "    h3 = self.gnorm3(h3)\n",
    "    h3 = self.act(h3)\n",
    "    h4 = self.conv4(h3)\n",
    "    h4 += self.dense4(embed)\n",
    "    h4 = self.gnorm4(h4)\n",
    "    h4 = self.act(h4)\n",
    "\n",
    "    # Decoding path\n",
    "    h = self.tconv4(h4)\n",
    "    ## Skip connection from the encoding path\n",
    "    h += self.dense5(embed)\n",
    "    h = self.tgnorm4(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "    h += self.dense6(embed)\n",
    "    h = self.tgnorm3(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "    h += self.dense7(embed)\n",
    "    h = self.tgnorm2(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "    # Normalize output\n",
    "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "    return h\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [],
   "source": [
    "class FLOW_MATCHING(nn.Module):\n",
    "\n",
    "    def __init__(self, network: nn.Module, device: torch.device, T: int = 100, ):\n",
    "        \"\"\"\n",
    "        Initialize Flow Matching model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the flow matching process. Typically a Unet.\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        device: torch.device\n",
    "            The device to run the model on.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(FLOW_MATCHING, self).__init__()\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (self._network(x.reshape(-1, 1, 28, 28), t)).reshape(-1, 28*28) # type: ignore\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad() # type: ignore\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample xT: Gaussian noise\n",
    "        xt = torch.randn(shape).to(self.device)\n",
    "\n",
    "        time_step_size = torch.tensor(1.0 / self.T).to(self.device)\n",
    "        for t in range(self.T):\n",
    "            # Scale time step to [0, 1]\n",
    "            time_step = torch.tensor([t / self.T]).to(self.device)\n",
    "\n",
    "            # Predict the velocity field\n",
    "            vt1 = self.network(xt, time_step) * time_step_size / 2\n",
    "            vt2 = self.network(xt + vt1, time_step + time_step_size / 2)\n",
    "            xt = xt + time_step_size * vt2\n",
    "\n",
    "        return xt\n",
    "\n",
    "    \n",
    "    def loss(self, x1: torch.tensor) -> float:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x1: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Flow value            \n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t uniformly from 0 to 1\n",
    "        t = torch.rand(1).to(self.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        x0 = torch.randn_like(x1)\n",
    "\n",
    "        xt = (1 - t) * x0 + t * x1\n",
    "        \n",
    "        return nn.MSELoss(reduction='mean')((x1 - x0), self.network(xt, t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 100/11800 [00:15<31:27,  6.20it/s, epoch=1/100, loss=⠀      0.7072, lr=9.90E-04]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 145\u001b[39m\n\u001b[32m    142\u001b[39m         plt.show()    \n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# Call training loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_epoch_callback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreporter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, scheduler, dataloader, epochs, device, ema, per_epoch_callback)\u001b[39m\n\u001b[32m     51\u001b[39m best_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     53\u001b[39m global_step_counter = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = _Image_fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PML/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "losses_list = []\n",
    "timings_list = []\n",
    "\n",
    "import time\n",
    "\n",
    "def train(model, optimizer, scheduler, dataloader, epochs, device, ema=True, per_epoch_callback=None):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(model, device=device, decay=1.0 - ema_alpha)                \n",
    "    \n",
    "\n",
    "    start_train_time = time.time()\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"⠀{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\", lr=f\"{scheduler.get_last_lr()[0]:.2E}\")\n",
    "            progress_bar.update()\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter%ema_steps==0:\n",
    "                    ema_model.update_parameters(model)                \n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            if epoch % 30 == 29:\n",
    "                per_epoch_callback(ema_model.module if ema else model)\n",
    "            \n",
    "        losses_list.append(best_loss)\n",
    "        timings_list.append(time.time() - start_train_time)\n",
    "\n",
    "# Parameters\n",
    "T = 500\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020, \n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),    # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x-0.5)*2.0),                    # Map from [0,1] -> [-1, -1]\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Construct Unet\n",
    "# The original ScoreNet expects a function with std for all the\n",
    "# different noise levels, such that the output can be rescaled.\n",
    "# Since we are predicting the noise (rather than the score), we\n",
    "# ignore this rescaling and just set std=1 for all t.\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = FLOW_MATCHING(mnist_unet, device, T=T).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "    \n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample((nsamples,28*28)).cpu()\n",
    "        \n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples+1)/2 \n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()    \n",
    "\n",
    "# Call training loop\n",
    "train(model, optimizer, scheduler, dataloader_train, \n",
    "      epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training losses\n",
    "plt.plot(losses_list)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Lowest loss per epoch\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training loss over time\n",
    "plt.plot(timings_list, losses_list)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Lowest loss over time\")\n",
    "plt.show()\n",
    "\n",
    "print(losses_list)\n",
    "print(timings_list)\n",
    "\n",
    "# Sample final model\n",
    "reporter(model)\n",
    "\n",
    "# show lowest loss per epoch\n",
    "print(f\"Lowest loss: {min(losses_list)}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "​",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:  24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": " 1393/5900 [05:15&lt;16:04,  4.67it/s, epoch=12/50, loss=⠀   2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
